{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021E0AF11CF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
          ]
        }
      ],
      "source": [
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf  # For loading your saved .keras model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model(\"yoga_pose_model.keras\")\n",
        "\n",
        "# Load the label encoder (you must save it during training)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = np.load('label_encoder_classes.npy')  # Load label encoder classes\n",
        "\n",
        "# Alias for MediaPipe\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# Set up PoseLandmarker\n",
        "model_path = 'pose_landmarker_lite.task'\n",
        "options = PoseLandmarkerOptions(\n",
        "    base_options=BaseOptions(model_asset_path=model_path),\n",
        "    running_mode=VisionRunningMode.IMAGE\n",
        ")\n",
        "\n",
        "# Load image for prediction\n",
        "image_path = \"C:/Users/HP/Desktop/SEM 6/Minor project 2/Book-Recommendation-System-master/Yoga Pose Detector/TRAIN/Veerabhadrasana/Images/Veerabhadrasana_4.jpg\"  # Your input image path\n",
        "test_img = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "# Ensure the image is valid\n",
        "if test_img is None:\n",
        "    print(f\"Skipping invalid image: {image_path}\")\n",
        "    exit()\n",
        "\n",
        "# Convert to RGB\n",
        "image_rgb = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convert to MediaPipe Image object\n",
        "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
        "\n",
        "with PoseLandmarker.create_from_options(options) as landmarker:\n",
        "    result = landmarker.detect(mp_image)\n",
        "\n",
        "    # If landmarks are detected\n",
        "    if result.pose_landmarks:\n",
        "        landmarks = result.pose_landmarks[0]  # Assuming single person in the image\n",
        "\n",
        "        # Flatten the landmarks and prepare feature vector for model\n",
        "        feature_vector = []\n",
        "        for lm in landmarks:\n",
        "            feature_vector.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
        "\n",
        "        # Convert feature vector to numpy array for prediction\n",
        "        X = np.array(feature_vector).reshape(1, -1)\n",
        "\n",
        "        # Predict the pose\n",
        "        predicted_pose = model.predict(X)\n",
        "        predicted_class = np.argmax(predicted_pose)  # Get the class index\n",
        "        pose_name = label_encoder.inverse_transform([predicted_class])[0]  # Decode the class index to pose name\n",
        "\n",
        "        # Draw landmarks on the image\n",
        "        for lm in landmarks:\n",
        "            x_px = int(lm.x * test_img.shape[1])\n",
        "            y_px = int(lm.y * test_img.shape[0])\n",
        "            cv2.circle(test_img, (x_px, y_px), 4, (0, 255, 0), -1)  # Green markers\n",
        "\n",
        "        # Write the detected pose on the image\n",
        "        cv2.putText(test_img, pose_name, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Display the image with markers and detected pose\n",
        "        cv2.imshow(\"Yoga Pose Detection\", test_img)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    else:\n",
        "        print(\"No pose landmarks detected.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x-qDM1RDAW3I"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "53TYfXuIAkFG",
        "outputId": "76052867-ce45-4ccf-b605-854bed9b9052"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\anaconda3\\envs\\pose_env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m17,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m520\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,602</span> (201.57 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,602\u001b[0m (201.57 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> (100.78 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,800\u001b[0m (100.78 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,802</span> (100.79 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m25,802\u001b[0m (100.79 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model=tf.keras.models.load_model('yoga_pose_model.keras')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dXsKTyQPCt31"
      },
      "outputs": [],
      "source": [
        "mediapipe_model_path='pose_landmarker_lite.task'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xG_Gg2b_AkKR"
      },
      "outputs": [],
      "source": [
        "BaseOptions = mp.tasks.BaseOptions\n",
        "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
        "PoseLandmarkerResult = mp.tasks.vision.PoseLandmarkerResult\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "base_options = python.BaseOptions(model_asset_path=mediapipe_model_path)\n",
        "KEY_LANDMARKS = [\n",
        "    mp.solutions.pose.PoseLandmark.NOSE,\n",
        "    mp.solutions.pose.PoseLandmark.LEFT_SHOULDER,\n",
        "    mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER,\n",
        "    mp.solutions.pose.PoseLandmark.LEFT_ELBOW,\n",
        "    mp.solutions.pose.PoseLandmark.RIGHT_ELBOW,\n",
        "    mp.solutions.pose.PoseLandmark.LEFT_HIP,\n",
        "    mp.solutions.pose.PoseLandmark.RIGHT_HIP\n",
        "]\n",
        "\n",
        "POSE_CLASSES = {\n",
        "    0: \"ArdhaChandrasana\",\n",
        "    1: \"BaddhaKonasana\",\n",
        "    2: \"Downward_dog\",\n",
        "    4:\"Natarajasana\",\n",
        "    5:\"Triangle\",\n",
        "    6:\"UtkataKonasana\",\n",
        "    7:\"Veerabhadrasana\",\n",
        "}\n",
        "\n",
        "\n",
        "# Create a pose landmarker instance with the live stream mode:\n",
        "def process_result(result: PoseLandmarkerResult, image: mp.Image, timestamp_ms: int):\n",
        "    image_array = image.numpy_view()\n",
        "    frame = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "    if result.pose_landmarks:# If the model sent landmarks\n",
        "        landmarks=[]\n",
        "        for lm in KEY_LANDMARKS: #Extract only key Landmarks\n",
        "            landmark = result.pose_landmarks[0][lm.value]\n",
        "            landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "            # Draw the landmark\n",
        "            h, w = image.height, image.width\n",
        "            cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
        "            cv2.circle(frame, (cx, cy), 5, (0, 255, 0), cv2.FILLED)\n",
        "\n",
        "        landmarks=np.array(landmarks).reshape(1,-1)\n",
        "        prediction=model.predict(landmarks)\n",
        "\n",
        "        predicted_class = np.argmax(prediction)\n",
        "        confidence = np.max(prediction)\n",
        "\n",
        "        if confidence > 0.7:\n",
        "            cv2.putText(frame,\n",
        "                       f\"{POSE_CLASSES.get(predicted_class, 'Unknown')}\",\n",
        "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            cv2.putText(frame,\n",
        "                       f\"Confidence: {confidence:.2f}\",\n",
        "                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('Yoga Pose Detection', frame)\n",
        "    cv2.waitKey(1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PCUqo3rBIgiY"
      },
      "outputs": [],
      "source": [
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=vision.RunningMode.LIVE_STREAM,\n",
        "    result_callback=process_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "# Initialize MediaPipe Vision and PoseLandmarker\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
        "\n",
        "# Path to model and image\n",
        "model_path = \"pose_landmarker_lite.task\"  # Make sure this file is in your working directory\n",
        "image_path = \"C:/Users/HP/OneDrive/Documents/SEM5/PA LAB/yoga/tree.jpg\"  # Replace with your image filename\n",
        "\n",
        "# Load the image using OpenCV and convert to RGB\n",
        "bgr_image = cv2.imread(image_path)\n",
        "rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Create a MediaPipe Image object\n",
        "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
        "\n",
        "# Set up the PoseLandmarker\n",
        "options = PoseLandmarkerOptions(\n",
        "    base_options=BaseOptions(model_asset_path=model_path),\n",
        "    running_mode=VisionRunningMode.IMAGE,\n",
        "    output_segmentation_masks=False\n",
        ")\n",
        "\n",
        "# Load and run the landmarker\n",
        "with PoseLandmarker.create_from_options(options) as landmarker:\n",
        "    result = landmarker.detect(mp_image)\n",
        "\n",
        "    # Draw landmarks\n",
        "    if result.pose_landmarks:\n",
        "        for landmark in result.pose_landmarks[0]:\n",
        "            x_px = int(landmark.x * bgr_image.shape[1])\n",
        "            y_px = int(landmark.y * bgr_image.shape[0])\n",
        "            cv2.circle(bgr_image, (x_px, y_px), 4, (0, 255, 0), -1)\n",
        "\n",
        "    # Show image with landmarks\n",
        "    cv2.imshow(\"Pose Landmarks\", bgr_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNEPefPcI1SC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Main loop\n",
        "if __name__ == \"__main__\":\n",
        "    with vision.PoseLandmarker.create_from_options(options) as landmarker:\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "            timestamp_ms = int(cv2.getTickCount() / cv2.getTickFrequency() * 1000)\n",
        "            landmarker.detect_async(mp_image, timestamp_ms)\n",
        "\n",
        "            if cv2.waitKey(5) & 0xFF == 27:  # ESC to exit\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pose_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
